---
title: "ce_scraping_bat_gather_urls"
description: Initial round of scraping to obtain list of auction result URLs from www.bringatrailer.com
output: html_document
date: 5/3/21
---

Load libraries
```{r}
library(tidyverse)
library(dplyr)
library(plotly)
library(ggplot2)
library(httr)
library(stringr)
library(rvest)
library(xml2)  #XML package no longer maintained. xml2 is a newer wrapper
#library(R.utils)
library(tidyjson)
```

Manually set user agent to be a very common one
```{r}
#set user agent 
ua <- "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36"

```

# PART I. 
# From BaT Makes & Models directory, fetch URLs for each unique make-model that has been listed on the site

```{r}
request <-GET("https://www.bringatrailer.com/models/"
              , user_agent(ua))
request$status_code
```

Save a list of model names
```{r}
model_names <- content(request)%>%
  html_nodes(xpath='//h2[@class="models-page-make-title"]//a')%>%
  html_text()

```

Fetch a list of all the make-model page  URLs
```{r}
model_urls<-content(request)%>%
  html_nodes(xpath='//div[@class="previous-listing-image-container"]/a[position()=2]')%>%
  html_attr('href')
```

Fetch a list of all the make-model labels to accompany the URLs
```{r}
model_names <- content(request)%>%
  html_nodes(xpath='//div[@class="previous-listing-image-container"]/a[position()=2]')%>%
  html_children%>%
  html_nodes(xpath='@alt')%>%
  html_text()

```

Bind the list of model_names and model_urls into a dataframe
```{r}
mm_df <- as.data.frame(do.call(cbind, list(model_urls,make_models)))%>%
  rename(model_url=V1
         ,model_name=V2)
```

Navigate to each model URL and fetch the specific model query id
```{r}
#Create column in dataframe and fill with placeholder values
#mm_df$query_id <- 'x'

#Fetch the query id for all models with for loop
for(i in 1:length(mm_df$model_url)) 
  
  if (mm_df$query_id[i]=="x")
    {
    #Call the vehicle's specific URL
    request <-GET(mm_df$model_url[i])
    
    #check the status code
    request$status_code
    
      #if request is successful, proceed to scraping the page
      if (request$status_code==200)
        {
          query_id <- content(request)%>%
            html_nodes(xpath='//link[@rel="shortlink"]')%>%
            html_node(xpath='@href')%>%
            html_text()%>%
            str_extract("\\d+")#%>%
            #as.numeric()
          
          #replace placeholder column value with the query id
          mm_df$query_id[i] <- query_id
        }
      
      #else if request returns anything other than a 200 'OK' status code, print the status code to the column values
      else 
        {
          #replace placeholder column values
          mm_df$query_id[i] <- request$status_code
        }
    
#    else
 #   {
#      mm_df$query_id[i] <- mm_df$query_id[i]
#    }
  
  #pause between requests & vary duration
  #generate sample within given range
  sleep_seconds <- sample(1:3,3) #(1:5,4)
  #
  Sys.sleep(sleep_seconds[(i-1) %% length(sleep_seconds)+1])
  
}

glimpse(mm_df)

view(mm_df)
```

Write to CSV
```{r}
#write.csv(mm_df, '../data/mmdf.csv')
```

# PART II(a). 
# For each model listed on BaT, use the query_id to collect the JSON objects with every individual vehicle's auction records
# Extract the number of results for each model and the number of pages of JSON results

Save total_results and count_pages to mm_df dataframe
```{r}
for(i in 1:length(mm_df$query_id))

  {
  #pause for 1 second between requests
  message("Getting page ", i)
  Sys.sleep(1)
  #every 10 requests, pause for 5 seconds
  if((i %% 10)==0){
    message("taking a break")
    Sys.sleep(5)
  }
  
    request <- GET(url = "https://bringatrailer.com/wp-json/bringatrailer/1.0/data/keyword-filter?",query = list(
                 bat_keyword_pages=mm_df$query_id[i],
                 sort='td',
                 page='1',
                 results='items'
               ),
               user_agent(ua))
    
    #check the status code
    request$status_code
    
      #if request is successful, proceed to scraping the page
      if (request$status_code==200)
        {
          
          #replace placeholder column value with the query id
          mm_df$total_results[i] <- content(request)$total
          
          mm_df$count_pages[i] <- content(request)$page_maximum

        }
      
      #else if request returns anything other than a 200 'OK' status code, print the status code to the column values
      else 
        {
          #replace placeholder column values
          mm_df$total_results[i] <- request$status_code
          mm_df$count_pages[i] <- request$status_code
        }
}

```

Update file saved to memory
```{r}
#write.csv(mm_df, '../data/mmdf.csv')
```

# Not all vehicle make pages return a JSON object with this pattern. Save this set separately. 

```{r}
mm_df$total_results%>%sum()

nrow(mm_df%>%filter(total_results==0))
```

Split mm_df into two dataframes
```{r}
#Make models with URLs that can be extracted from JSON objects
mm_df_json <- mm_df%>%
  filter(count_pages!=0)%>%
  mutate(http_error="") #create column to store any response errors

#write.csv(mm_df_json, '../data/mm_df_json.csv', row.names = FALSE)

#Models with URLs that need to be wrangled with html
mm_df_json_b <- mm_df%>%
  filter(count_pages==0)%>%
  mutate(http_error="")

#write.csv(mm_df_json_b, '../data/mm_df_json_b.csv', row.names = FALSE)
```

# PART II(b)

# Loop over mm_df_json to get the URLs of vehicle listings from the JSON objects

```{r}

for(i in 1:length(mm_df_json$query_id))
{
  for(n in 1:mm_df_json$count_pages[i])
  {
    #Print status message
    message("Fetching result set ", n, 
            " for query count ",i, 
            " out of ", length(mm_df_json$query_id))
    
    #Pause for 1 second between requests
    Sys.sleep(1)
    
    #Every 10 requests, pause for 5 seconds
    if((i %% 10)==0){
      message("Taking a break")
      Sys.sleep(3)}
  
    request <- GET(url = "https://bringatrailer.com/wp-json/bringatrailer/1.0/data/keyword-filter?",
               query = list(
                 bat_keyword_pages=mm_df_json$query_id[i],
                 sort='td',
                 page=n,
                 results='items'
               ),
               user_agent(ua))
    
      #Check the status code
      #Request is NOT successful
      if (request$status_code!=200)
        {
          #Return a message in the console 
          message("The attempt to fetch result set ", n, 
              " for query count ",i, 
              " resulted in error code ", request$status_code)
        
          #Store the query id and the error response codes for diagnostics
          mm_df_json$http_error <- mm_df_json$query_id[i]
        }
      
      #Request IS successful
      else 
        {
          #Parse the JSON object using tidyjson & save as a table
          result_df <- content(request)$items%>%
            enter_object%>% # enter into that object, gather&stack array
            spread_all()%>% # capture the object items under the array
            as.tbl_json()%>%
            select(id, url, title, subtitle)
          
          #Map the query_id to the results dataframe
          result_df$query_id <- mm_df_json$query_id[i]
          
            #Create a dataframe with the very first response set
            if (i==1 & n==1)
            {
              auction_urls_a <- result_df
            }
            
            #Append every other response set to that dataframe
            else 
            {
              #append the result set to the master table
              auction_urls_a <- rbind(auction_urls_a,result_df)
            }

        }
  }
  
#  return(auction_urls)
  }
```

Check to make sure all values are as expected
```{r}
#Are the total results the same as the unique auction ids?
sum(mm_df_json$total_results)

n_distinct(auction_urls_a$id)

#Double check total results vs. total unique auction ids grouped by query_id
left_join(
  auction_urls_a%>%
    group_by(query_id)%>%
    summarise(count=n_distinct(id))%>%
    ungroup(),
  
  mm_df_json%>%
    select(query_id,total_results),
)
```

What vehicles appear more than once in this list?
**Some vehicles are associated with more than one query_id
eg: Ford F-150 SVT Lightning is tied to two model query_ids: one for Ford F-150 SVT Lightning and one for Ford Pickup
```{r}
sort(table(auction_urls_a$id), decreasing = TRUE)
```

Save to CSV - cast to regular dataframe and select only relevant columns
```{r}
#write.csv(auction_urls_a%>%
#          as.data.frame()%>%
#          select(id, url, title, subtitle, query_id)
#          , '../data/auction_urls_aa.csv'
#          ,row.names = FALSE)
```

# PART II(c). 
# Not all vehicle make page queries return a JSON object. 41 rows in mm_df that returned a value of zero in total_results and count_pages because the query parameters for certain models are formatted differently.

data-query_args=”{“bat_keyword_pages”:[183300]}” for BMW E34 M5 listings
vs.
data-query_args="{“s”:”Packard”}" for Packard listings

Validate this different query patter
```{r}
request<- GET("https://bringatrailer.com/hummer/")

#Part of the auction results are stored in a JSON object in the page code

model_results <- content(request)%>%
  html_nodes(xpath='//div[@class="filter-group"]')%>%
  html_attr('data-list')

#Attached to this object is the data-query_args key-value pair

#Key
(content(request)%>%
  html_nodes(xpath='//div[@class="filter-group"]')%>%
  html_attr('data-query_args')%>%
  gather_object()%>%
  json_types()%>%
  as_data_frame.tbl_json()%>%
  select(name)
  )[[1,1]]


#Value
(content(request)%>%
  html_nodes(xpath='//div[@class="filter-group"]')%>%
  html_attr('data-query_args')%>%
  enter_object()%>%
  spread_all()%>%
  as_data_frame.tbl_json()%>%
  select(s)
  )[[1,1]]
```
```{r}
#mm_df_json_b <- read_csv('../data/mm_df_json_b.csv')
```
Save total_results and count_pages to mm_df_json_b dataframe
```{r}
for(i in 1:length(mm_df_json_b$query_id))

  {
  #pause for 1 second between requests
  message("Getting page ", i)
  Sys.sleep(1)
  #every 10 requests, pause for 5 seconds
  if((i %% 10)==0){
    message("taking a break")
    Sys.sleep(3)
  }
  
    request <- GET(url = "https://bringatrailer.com/wp-json/bringatrailer/1.0/data/keyword-filter?",query = list(
                 s=mm_df_json_b$model_name[i]%>%
                   str_replace(" ","-"),
                 sort='td',
                 page='1',
                 results='items'
               ),
               user_agent(ua))

      #if request is successful, proceed to scraping the page
      if (request$status_code==200)
        {
          
          #replace placeholder column value with the query id
          mm_df_json_b$total_results[i] <- content(request)$total
          
          mm_df_json_b$count_pages[i] <- content(request)$page_maximum

        }
      
      #else if request returns anything other than a 200 'OK' status code, print the status code to the column values
      else 
        {
          #replace placeholder column values
          mm_df_json_b$total_results[i] <- request$status_code
          mm_df_json_b$count_pages[i] <- request$status_code
        }
}
```
Save updated mm_df_json_b to csv
```{r}
#write.csv(mm_df_json_b%>%
#          select(!http_error)
#          , '../data/mm_df_json_b.csv'
#          ,row.names = FALSE)
```

Use the total_results and count_pages values to get a list of target_urls for the models in mm_df_json_b
```{r}
for(i in 1:length(mm_df_json_b$query_id))
{
  for(n in 1:mm_df_json_b$count_pages[i])
  {
    #Print status message
    message("Fetching result set ", n, 
            " for query count ",i, 
            " out of ", length(mm_df_json_b$query_id))
    
    #Pause for 1 second between requests
    Sys.sleep(1)
    
    #Every 10 requests, pause for 5 seconds
    if((i %% 10)==0){
      message("Taking a break")
      Sys.sleep(3)}
  
    request <- GET(url = "https://bringatrailer.com/wp-json/bringatrailer/1.0/data/keyword-filter?",
               query = list(
                 s=mm_df_json_b$model_name[i]%>%
                   str_replace(" ","-"),
                 sort='td',
                 page=n,
                 results='items'
               ),
               user_agent(ua))
    
      #Check the status code
      #Request is NOT successful
      if (request$status_code!=200)
        {
          #Return a message in the console 
          message("The attempt to fetch result set ", n, 
              " for query count ",i, 
              " resulted in error code ", request$status_code)
        
          #Store the query id and the error response codes for diagnostics
          #mm_df_json_b$http_error[i] <- request$status_code
        }
      
      #Request IS successful
      else 
        {
          #Parse the JSON object using tidyjson & save as a table
          result_df <- content(request)$items%>%
            enter_object%>% # enter into that object, gather&stack array
            spread_all()%>% # capture the object items under the array
            as.tbl_json()%>%
            select(id, url, title, subtitle)
          
          #Map the query_id to the results dataframe
          result_df$query_id <- mm_df_json_b$query_id[i]
          
            #Create a dataframe with the very first response set
            if (i==1 & n==1)
            {
              auction_urls_b <- result_df
            }
            
            #Append every other response set to that dataframe
            else 
            {
              #append the result set to the master table
              auction_urls_b <- rbind(auction_urls_b,result_df)
            }

        }
  }
  
#  return(auction_urls)
  }
```

Save to CSV - cast to regular dataframe and select only relevant columns
```{r}
#write.csv(auction_urls_b%>%
#          as.data.frame()%>%
#          select(id, url, title, subtitle, query_id)
#          , '../data/auction_urls_b.csv'
#          ,row.names = FALSE)

#auction_urls_b <- read_csv('../data/auction_urls_b.csv')
```

# Determine what pages were missed in initial batch of scaping
# First pass at scraping (in notebook titled ce_scraping_http_to_rmd) did not capture every single response

There are 41,895 rows in the auction_urls_a and all 41,895 target urls were queried.  However there are only 40,210 unique auction ids.  This is because auction_urls_a includes the query_id, and a vehicle may be associated with more than one make-model category. 
```{r}
n_distinct(auction_urls_a$id)

nrow(auction_urls_a)
```
However, the number of .rds files saved is different from the number of files expected. 40,588 were saved. 

Check how many auction ids appeared more than once
```{r}
id_multi_rows <- as.data.frame(table(auction_urls_a$id), #row.names = NULL,
              responseName = "Freq", #stringsAsFactors = TRUE,
              #sep = "", base = list(LETTERS)
              )%>%
  filter(Freq >1)

n_distinct(id_multi_rows$Var1)
```
Examine what kinds of auctions have multiple records in this list
```{r}
id_multi_detail <- auction_urls_a%>%
  filter(id %in% id_multi_rows$Var1)

n_distinct(id_multi_detail$id)
n_distinct(id_multi_detail$url)
```

# Obtain a list of file names from the .rds files
```{r}
file_list <- list.files(path = "../data/bat_page_responses/group_a/", pattern = NULL, all.files = FALSE,
           full.names = FALSE, recursive = TRUE,
           ignore.case = FALSE, include.dirs = FALSE, no.. = FALSE)
```
```{r}

file_list%>%
  str_remove_all(".rds")%>%
  str_replace_all("(rows_\\d+_\\d+/)","")
```


```{r}
missing_files <- auction_urls_b%>%
  filter(id %in% (file_list%>%
                     str_remove_all(".rds")%>%
                     str_replace_all("(rows_\\d+_\\d+/)","")
                   )
         )
missing_files
```

# Check MASTER LIST of auction results
<https://bringatrailer.com/auctions/results/>
<https://bringatrailer.com/wp-json/bringatrailer/1.0/data/keyword-filter?page=1&sort=td&results=items>

```{r}
for(i in 1:1527)
{
#  for(n in 1:mm_df_json_b$count_pages[i])
  {
    #Print status message
    message("Fetching result set ", i, 
            " out of ", 1527)
    
    #Pause for 1 second between requests
    Sys.sleep(1)
    
    #Every 10 requests, pause for 2 seconds
    if((i %% 10)==0){
      message("Taking a break")
      Sys.sleep(2)}
  
    request <- GET(url = "https://bringatrailer.com/wp-json/bringatrailer/1.0/data/keyword-filter?",
               query = list(
                 sort='td',
                 page=i,
                 results='items'
               ),
               user_agent(ua))
    
      #Check the status code
      #Request is NOT successful
      if (request$status_code!=200)
        {
          #Return a message in the console 
          message("The attempt to fetch result set ", i, 
              " resulted in error code ", request$status_code)
        }
      
      #Request IS successful
      else 
        {
          #Parse the JSON object using tidyjson & save as a table
          result_df <- content(request)$items%>%
            enter_object%>% # enter into that object, gather&stack array
            spread_all()%>% # capture the object items under the array
            as.tbl_json()%>%
            select(id, url, title, subtitle)
          
            #Create a dataframe with the very first response set
            if (i==1)
            {
              auction_urls_master <- result_df
            }
            
            #Append every other response set to that dataframe
            else 
            {
              #append the result set to the master table
              auction_urls_master <- rbind(auction_urls_master,result_df)
            }

        }
  }
  
  }
```
```{r}
#view(auction_urls_master)
auction_urls_master_312 <- auction_urls_master
```

Save to CSV - cast to regular dataframe and select only relevant columns
```{r}
#write.csv(auction_urls_master_312%>%
#          as.data.frame()%>%
#          select(id, url, title, subtitle)
#          , '../data/auction_urls_master_312.csv'
#          ,row.names = FALSE)

#auction_urls_b <- read_csv('../data/auction_urls_b.csv')
```












